"""
å†…å®¹æ–°é²œåº¦ç›‘æ§ç³»ç»Ÿ
æ£€æµ‹é‡å¤ç‡ã€åˆ›æ„æ¯ç«­ï¼Œå¹¶æé†’éœ€è¦æ–°ç´ æ
"""

import json
from datetime import datetime, timedelta
from pathlib import Path
from collections import Counter
from difflib import SequenceMatcher
from ..core.logger import setup_logger

logger = setup_logger('freshness_monitor')


class ContentFreshnessMonitor:
    """ç›‘æ§ç”Ÿæˆå†…å®¹çš„æ–°é²œåº¦å’Œé‡å¤ç‡"""

    def __init__(self, history_file: str = None):
        """
        Args:
            history_file: å†å²è®°å½•æ–‡ä»¶è·¯å¾„
        """
        if history_file is None:
            history_file = Path(__file__).parent.parent.parent / 'data' / 'generated_history.json'

        self.history_file = Path(history_file)
        self.history_file.parent.mkdir(parents=True, exist_ok=True)

        self.history = self._load_history()

        # é˜ˆå€¼é…ç½®
        self.THRESHOLDS = {
            'exact_duplicate_rate': 0.10,      # 10% å®Œå…¨é‡å¤å°±æŠ¥è­¦
            'similar_duplicate_rate': 0.25,    # 25% ç›¸ä¼¼é‡å¤å°±æŠ¥è­¦
            'phrase_reuse_rate': 0.40,         # 40% çŸ­è¯­é‡å¤å°±æŠ¥è­¦
            'days_since_training': 30,         # 30 å¤©æ²¡è®­ç»ƒå°±æé†’
            'content_staleness_score': 0.50,   # æ–°é²œåº¦ä½äº 0.50 å¼€å§‹æŠ¥è­¦
            'high_severity_threshold': 0.35,   # æ–°é²œåº¦ä½äº 0.35 ä¸ºé«˜ä¸¥é‡åº¦
            'medium_severity_threshold': 0.50  # æ–°é²œåº¦ä½äº 0.50 ä¸ºä¸­ç­‰ä¸¥é‡åº¦
        }

    def _load_history(self) -> dict:
        """åŠ è½½å†å²è®°å½•"""
        if self.history_file.exists():
            with open(self.history_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            return {
                'generated_posts': [],
                'training_data_updates': [],
                'alerts': [],
                'stats': {
                    'total_generated': 0,
                    'last_training_date': None
                }
            }

    def _save_history(self):
        """ä¿å­˜å†å²è®°å½•"""
        with open(self.history_file, 'w', encoding='utf-8') as f:
            json.dump(self.history, f, ensure_ascii=False, indent=2)

    def record_generated_post(self, post_text: str, content_type: str, metadata: dict = None):
        """è®°å½•ç”Ÿæˆçš„æ¨æ–‡

        Args:
            post_text: æ¨æ–‡å†…å®¹
            content_type: 'gm', 'main', 'casual', etc.
            metadata: é¢å¤–ä¿¡æ¯ï¼ˆtheme, day, etc.ï¼‰
        """
        record = {
            'id': len(self.history['generated_posts']) + 1,
            'text': post_text,
            'content_type': content_type,
            'timestamp': datetime.now().isoformat(),
            'metadata': metadata or {}
        }

        self.history['generated_posts'].append(record)
        self.history['stats']['total_generated'] += 1

        self._save_history()
        logger.info(f"Recorded post #{record['id']}: {post_text[:50]}...")

    def record_training_update(self, training_type: str, samples_added: int, notes: str = ""):
        """è®°å½•è®­ç»ƒæ•°æ®æ›´æ–°

        Args:
            training_type: 'gm', 'main', 'reply', etc.
            samples_added: æ·»åŠ çš„æ ·æœ¬æ•°é‡
            notes: å¤‡æ³¨
        """
        record = {
            'type': training_type,
            'samples_added': samples_added,
            'date': datetime.now().isoformat(),
            'notes': notes
        }

        self.history['training_data_updates'].append(record)
        self.history['stats']['last_training_date'] = datetime.now().isoformat()

        self._save_history()
        logger.info(f"Recorded training update: {training_type} (+{samples_added} samples)")

    def check_freshness(self, content_type: str = 'gm', recent_window: int = 50) -> dict:
        """æ£€æŸ¥å†…å®¹æ–°é²œåº¦

        Args:
            content_type: æ£€æŸ¥çš„å†…å®¹ç±»å‹
            recent_window: æ£€æŸ¥æœ€è¿‘ N æ¡

        Returns:
            {
                'is_fresh': bool,
                'alerts': list,
                'stats': dict,
                'recommendations': list
            }
        """
        # ç­›é€‰ç›¸å…³ç±»å‹çš„æ¨æ–‡
        posts = [p for p in self.history['generated_posts'] if p['content_type'] == content_type]
        recent_posts = posts[-recent_window:] if len(posts) > recent_window else posts

        if len(recent_posts) < 10:
            return {
                'is_fresh': True,
                'alerts': [],
                'stats': {'message': 'Not enough data to analyze'},
                'recommendations': []
            }

        # 1. æ£€æŸ¥å®Œå…¨é‡å¤
        exact_duplicates = self._check_exact_duplicates(recent_posts)

        # 2. æ£€æŸ¥ç›¸ä¼¼é‡å¤
        similar_duplicates = self._check_similar_duplicates(recent_posts)

        # 3. æ£€æŸ¥çŸ­è¯­é‡å¤
        phrase_reuse = self._check_phrase_reuse(recent_posts)

        # 4. æ£€æŸ¥è·ç¦»ä¸Šæ¬¡è®­ç»ƒçš„æ—¶é—´
        training_staleness = self._check_training_staleness()

        # 5. è®¡ç®—ç»¼åˆæ–°é²œåº¦åˆ†æ•°
        freshness_score = self._calculate_freshness_score(
            exact_duplicates,
            similar_duplicates,
            phrase_reuse,
            training_staleness
        )

        # ç”ŸæˆæŠ¥è­¦
        alerts = []
        recommendations = []

        if exact_duplicates['rate'] > self.THRESHOLDS['exact_duplicate_rate']:
            alerts.append({
                'severity': 'HIGH',
                'type': 'exact_duplicate',
                'message': f"âš ï¸ å®Œå…¨é‡å¤ç‡è¿‡é«˜: {exact_duplicates['rate']*100:.1f}% (é˜ˆå€¼: {self.THRESHOLDS['exact_duplicate_rate']*100:.0f}%)",
                'details': exact_duplicates
            })
            recommendations.append("ç«‹å³æ·»åŠ æ–°çš„è®­ç»ƒæ ·æœ¬ï¼Œé¿å…ç”Ÿæˆé‡å¤å†…å®¹")

        if similar_duplicates['rate'] > self.THRESHOLDS['similar_duplicate_rate']:
            alerts.append({
                'severity': 'MEDIUM',
                'type': 'similar_duplicate',
                'message': f"âš ï¸ ç›¸ä¼¼é‡å¤ç‡è¿‡é«˜: {similar_duplicates['rate']*100:.1f}% (é˜ˆå€¼: {self.THRESHOLDS['similar_duplicate_rate']*100:.0f}%)",
                'details': similar_duplicates
            })
            recommendations.append("å†…å®¹å¼€å§‹å¥—è·¯åŒ–ï¼Œå»ºè®®ä¸°å¯Œè¯æ±‡åº“å’Œè¡¨è¾¾æ–¹å¼")

        if phrase_reuse['rate'] > self.THRESHOLDS['phrase_reuse_rate']:
            alerts.append({
                'severity': 'MEDIUM',
                'type': 'phrase_reuse',
                'message': f"âš ï¸ çŸ­è¯­é‡å¤ç‡è¿‡é«˜: {phrase_reuse['rate']*100:.1f}% (é˜ˆå€¼: {self.THRESHOLDS['phrase_reuse_rate']*100:.0f}%)",
                'details': phrase_reuse
            })
            recommendations.append(f"é«˜é¢‘çŸ­è¯­: {', '.join(phrase_reuse['top_phrases'][:5])}")

        if training_staleness['days_since_training'] > self.THRESHOLDS['days_since_training']:
            alerts.append({
                'severity': 'LOW',
                'type': 'training_staleness',
                'message': f"ğŸ“… è·ç¦»ä¸Šæ¬¡è®­ç»ƒå·²è¿‡ {training_staleness['days_since_training']} å¤©",
                'details': training_staleness
            })
            recommendations.append("å®šæœŸè¡¥å……æ–°ç´ æå¯ä»¥ä¿æŒå†…å®¹æ–°é²œåº¦")

        if freshness_score < self.THRESHOLDS['content_staleness_score']:
            alerts.append({
                'severity': 'HIGH',
                'type': 'content_staleness',
                'message': f"ğŸš¨ å†…å®¹æ–°é²œåº¦è¿‡ä½: {freshness_score:.2f} (é˜ˆå€¼: {self.THRESHOLDS['content_staleness_score']:.2f})",
                'details': {'score': freshness_score}
            })
            recommendations.append("âš ï¸ ç´§æ€¥ï¼šéœ€è¦ç«‹å³è¡¥å……æ–°è®­ç»ƒç´ æï¼")

        # è®°å½•æŠ¥è­¦
        if alerts:
            alert_record = {
                'timestamp': datetime.now().isoformat(),
                'content_type': content_type,
                'alerts': alerts,
                'freshness_score': freshness_score
            }
            self.history['alerts'].append(alert_record)
            self._save_history()

        return {
            'is_fresh': freshness_score >= self.THRESHOLDS['content_staleness_score'] and len(alerts) == 0,
            'freshness_score': freshness_score,
            'alerts': alerts,
            'stats': {
                'exact_duplicate_rate': exact_duplicates['rate'],
                'similar_duplicate_rate': similar_duplicates['rate'],
                'phrase_reuse_rate': phrase_reuse['rate'],
                'days_since_training': training_staleness['days_since_training'],
                'total_posts_analyzed': len(recent_posts)
            },
            'recommendations': recommendations
        }

    def _check_exact_duplicates(self, posts: list) -> dict:
        """æ£€æŸ¥å®Œå…¨é‡å¤"""
        texts = [p['text'].strip().lower() for p in posts]
        counter = Counter(texts)
        duplicates = {text: count for text, count in counter.items() if count > 1}

        duplicate_rate = sum(count - 1 for count in duplicates.values()) / len(texts) if texts else 0

        return {
            'rate': duplicate_rate,
            'count': len(duplicates),
            'examples': list(duplicates.items())[:5]
        }

    def _check_similar_duplicates(self, posts: list, similarity_threshold: float = 0.8) -> dict:
        """æ£€æŸ¥ç›¸ä¼¼é‡å¤ï¼ˆä½¿ç”¨ç¼–è¾‘è·ç¦»ï¼‰"""
        texts = [p['text'].strip().lower() for p in posts]
        similar_pairs = []

        for i in range(len(texts)):
            for j in range(i + 1, len(texts)):
                similarity = SequenceMatcher(None, texts[i], texts[j]).ratio()
                if similarity >= similarity_threshold:
                    similar_pairs.append((texts[i], texts[j], similarity))

        similar_rate = len(similar_pairs) / len(texts) if texts else 0

        return {
            'rate': similar_rate,
            'count': len(similar_pairs),
            'examples': similar_pairs[:5]
        }

    def _check_phrase_reuse(self, posts: list, min_phrase_length: int = 3) -> dict:
        """æ£€æŸ¥çŸ­è¯­é‡å¤ï¼ˆ2-4 ä¸ªè¯çš„ç»„åˆï¼‰"""
        all_phrases = []

        for post in posts:
            words = post['text'].lower().split()
            # æå– 2-4 è¯çš„çŸ­è¯­
            for n in range(2, 5):
                for i in range(len(words) - n + 1):
                    phrase = ' '.join(words[i:i+n])
                    # è¿‡æ»¤æ‰å¤ªå¸¸è§çš„è¯ï¼ˆgm, from, the ç­‰ï¼‰
                    if phrase not in ['gm from', 'from the', 'the data', 'data labeling']:
                        all_phrases.append(phrase)

        if not all_phrases:
            return {'rate': 0, 'count': 0, 'top_phrases': []}

        counter = Counter(all_phrases)
        repeated_phrases = {phrase: count for phrase, count in counter.items() if count > 2}

        reuse_rate = len(repeated_phrases) / len(set(all_phrases)) if all_phrases else 0

        return {
            'rate': reuse_rate,
            'count': len(repeated_phrases),
            'top_phrases': [phrase for phrase, count in counter.most_common(10) if count > 2]
        }

    def _check_training_staleness(self) -> dict:
        """æ£€æŸ¥è·ç¦»ä¸Šæ¬¡è®­ç»ƒçš„æ—¶é—´"""
        last_training = self.history['stats'].get('last_training_date')

        if not last_training:
            return {
                'days_since_training': 999,
                'last_training_date': None,
                'message': 'ä»æœªè®°å½•è¿‡è®­ç»ƒæ›´æ–°'
            }

        last_date = datetime.fromisoformat(last_training)
        days_since = (datetime.now() - last_date).days

        return {
            'days_since_training': days_since,
            'last_training_date': last_training,
            'message': f'ä¸Šæ¬¡è®­ç»ƒ: {days_since} å¤©å‰'
        }

    def _calculate_freshness_score(self, exact_dup, similar_dup, phrase_reuse, training_stale) -> float:
        """è®¡ç®—ç»¼åˆæ–°é²œåº¦åˆ†æ•° (0-1, 1 æœ€æ–°é²œ)"""

        # å„é¡¹æƒé‡
        weights = {
            'exact_duplicate': 0.4,
            'similar_duplicate': 0.3,
            'phrase_reuse': 0.2,
            'training_staleness': 0.1
        }

        # è®¡ç®—å„é¡¹å¾—åˆ†ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
        exact_score = max(0, 1 - exact_dup['rate'] / self.THRESHOLDS['exact_duplicate_rate'])
        similar_score = max(0, 1 - similar_dup['rate'] / self.THRESHOLDS['similar_duplicate_rate'])
        phrase_score = max(0, 1 - phrase_reuse['rate'] / self.THRESHOLDS['phrase_reuse_rate'])
        training_score = max(0, 1 - training_stale['days_since_training'] / self.THRESHOLDS['days_since_training'])

        # åŠ æƒå¹³å‡
        total_score = (
            exact_score * weights['exact_duplicate'] +
            similar_score * weights['similar_duplicate'] +
            phrase_score * weights['phrase_reuse'] +
            training_score * weights['training_staleness']
        )

        return total_score

    def get_freshness_report(self, content_type: str = 'gm') -> str:
        """ç”Ÿæˆæ–°é²œåº¦æŠ¥å‘Šï¼ˆé€‚åˆå‘é€é€šçŸ¥ï¼‰"""
        result = self.check_freshness(content_type)

        if result['is_fresh']:
            return f"âœ… {content_type.upper()} å†…å®¹æ–°é²œåº¦è‰¯å¥½ (å¾—åˆ†: {result['freshness_score']:.2f})"

        report_lines = [
            f"ğŸ“Š {content_type.upper()} å†…å®¹æ–°é²œåº¦æŠ¥å‘Š",
            f"å¾—åˆ†: {result['freshness_score']:.2f} / 1.00",
            "",
            "âš ï¸ é—®é¢˜ï¼š"
        ]

        for alert in result['alerts']:
            severity_emoji = {
                'HIGH': 'ğŸš¨',
                'MEDIUM': 'âš ï¸',
                'LOW': 'ğŸ“…'
            }
            report_lines.append(f"{severity_emoji[alert['severity']]} {alert['message']}")

        if result['recommendations']:
            report_lines.append("")
            report_lines.append("ğŸ’¡ å»ºè®®ï¼š")
            for rec in result['recommendations']:
                report_lines.append(f"  - {rec}")

        report_lines.append("")
        report_lines.append("ğŸ“ˆ ç»Ÿè®¡ï¼š")
        report_lines.append(f"  - å®Œå…¨é‡å¤ç‡: {result['stats']['exact_duplicate_rate']*100:.1f}%")
        report_lines.append(f"  - ç›¸ä¼¼é‡å¤ç‡: {result['stats']['similar_duplicate_rate']*100:.1f}%")
        report_lines.append(f"  - çŸ­è¯­é‡å¤ç‡: {result['stats']['phrase_reuse_rate']*100:.1f}%")
        report_lines.append(f"  - è·ä¸Šæ¬¡è®­ç»ƒ: {result['stats']['days_since_training']} å¤©")

        return "\n".join(report_lines)

    def auto_check_and_alert(self, content_type: str = 'gm', check_interval: int = 20) -> dict:
        """è‡ªåŠ¨æ£€æŸ¥å¹¶è¿”å›æ˜¯å¦éœ€è¦æŠ¥è­¦

        Args:
            content_type: å†…å®¹ç±»å‹
            check_interval: æ¯ç”Ÿæˆ N æ¡å°±æ£€æŸ¥ä¸€æ¬¡

        Returns:
            {
                'should_alert': bool,
                'message': str
            }
        """
        posts = [p for p in self.history['generated_posts'] if p['content_type'] == content_type]

        # æ¯ N æ¡æ£€æŸ¥ä¸€æ¬¡
        if len(posts) % check_interval == 0 and len(posts) > 0:
            result = self.check_freshness(content_type)

            if not result['is_fresh']:
                return {
                    'should_alert': True,
                    'message': self.get_freshness_report(content_type)
                }

        return {
            'should_alert': False,
            'message': ''
        }


# ä¾¿æ·å‡½æ•°
def check_content_freshness(content_type: str = 'gm') -> dict:
    """å¿«é€Ÿæ£€æŸ¥å†…å®¹æ–°é²œåº¦"""
    monitor = ContentFreshnessMonitor()
    return monitor.check_freshness(content_type)


def get_freshness_report(content_type: str = 'gm') -> str:
    """è·å–æ–°é²œåº¦æŠ¥å‘Š"""
    monitor = ContentFreshnessMonitor()
    return monitor.get_freshness_report(content_type)


# æµ‹è¯•
if __name__ == '__main__':
    monitor = ContentFreshnessMonitor()

    # æ¨¡æ‹Ÿç”Ÿæˆä¸€äº›é‡å¤å†…å®¹
    print("ğŸ§ª æ¨¡æ‹Ÿç”Ÿæˆ 30 æ¡ GM postsï¼ˆåŒ…å«é‡å¤ï¼‰...\n")

    test_posts = [
        "gm from the trenches",
        "gm",
        "gm ğŸ¥±",
        "gm from the trenches",  # é‡å¤
        "gm builders",
        "gm from the data mines",
        "gm",  # é‡å¤
        "gm from the trenches",  # é‡å¤
        "gm debugging",
        "gm from the void",
    ] * 3  # é‡å¤ 3 é

    for post in test_posts:
        monitor.record_generated_post(post, 'gm', {'theme': 'test'})

    print(f"âœ… å·²è®°å½• {len(test_posts)} æ¡ GM posts\n")
    print("=" * 70)

    # æ£€æŸ¥æ–°é²œåº¦
    print("\n" + monitor.get_freshness_report('gm'))
